{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9f18019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('/home/as2114/code/PlantTraitNet/src')\n",
    "from utils.benchmark_conf import get_benchmark_config\n",
    "from utils.benchmark_utils import read_trait_map, global_grid_df, lat_weights\n",
    "\n",
    "cfg = get_benchmark_config()\n",
    "# -------------------- METRICS --------------------\n",
    "def weighted_pearsonr(x, y, w):\n",
    "    mean_x, mean_y = np.average(x, weights=w), np.average(y, weights=w)\n",
    "    cov_xy = np.sum(w * (x - mean_x) * (y - mean_y))\n",
    "    return cov_xy / np.sqrt(np.sum(w * (x - mean_x) ** 2) * np.sum(w * (y - mean_y) ** 2))\n",
    "\n",
    "\n",
    "def compute_weighted_metrics(df, latwts, pred_col, true_col, log_scale=False):\n",
    "    \"\"\"Compute weighted metrics between prediction and ground truth\"\"\"\n",
    "    df = df.copy()\n",
    "    if df.index.names != [\"y\", \"x\"]:\n",
    "        df.index.set_names([\"y\", \"x\"], inplace=True)\n",
    "\n",
    "    df[\"weight\"] = df.index.get_level_values(\"y\").map(latwts)\n",
    "    df = df.dropna(subset=[\"weight\", pred_col, true_col])\n",
    "\n",
    "    y_true, y_pred, w = df[true_col].values, df[pred_col].values, df[\"weight\"].values\n",
    "    if log_scale:\n",
    "        y_true, y_pred = np.log1p(y_true), np.log1p(y_pred)\n",
    "\n",
    "    if len(y_true) == 0:\n",
    "        return dict.fromkeys([\"n\", \"mae\", \"nmae\", \"rmse\", \"nrmse\", \"r2\", \"slope\", \"pearson_r\"], np.nan)\n",
    "\n",
    "    err = y_pred - y_true\n",
    "    mae = np.average(np.abs(err), weights=w)\n",
    "    rmse = np.sqrt(np.average(err ** 2, weights=w))\n",
    "    rng = np.quantile(y_true, 0.99) - np.quantile(y_true, 0.01)\n",
    "    nmae, nrmse = mae / rng, rmse / rng\n",
    "\n",
    "    ss_res, ss_tot = np.sum(w * err ** 2), np.sum(w * (y_true - np.average(y_true, weights=w)) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else np.nan\n",
    "    slope, _, *_ = linregress(y_true, y_pred)\n",
    "    r = weighted_pearsonr(y_true, y_pred, w)\n",
    "    return dict(n=len(df), mae=mae, nmae=nmae, rmse=rmse, nrmse=nrmse, r2=r2, slope=slope, pearson_r=r)\n",
    "\n",
    "\n",
    "def plothexbin(df, true_col, pred_col, latwts, ax, label=None,\n",
    "               scaleaxis=True, allmetrics=True, n_min=1, log_scale=False, title=''):\n",
    "    \"\"\"Plot weighted hexbin with regression metrics overlay\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"⚠️ Empty DataFrame passed to plothexbin\")\n",
    "        return ax\n",
    "\n",
    "    metrics = compute_weighted_metrics(df, latwts, pred_col, true_col, log_scale=log_scale)\n",
    "    y_true, y_pred = df[true_col].values, df[pred_col].values\n",
    "    if log_scale:\n",
    "        y_true, y_pred = np.log1p(y_true), np.log1p(y_pred)\n",
    "\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "    if scaleaxis:\n",
    "        maxval, minval = max(y_true.max(), y_pred.max()), min(y_true.min(), y_pred.min())\n",
    "        ax.set_xlim(minval, maxval)\n",
    "        ax.set_ylim(minval, maxval)\n",
    "        ax.plot([minval, maxval], [minval, maxval], color='red', lw=2)\n",
    "        extent = [minval, maxval, minval, maxval]\n",
    "    else:\n",
    "        extent = None\n",
    "\n",
    "    hb = ax.hexbin(y_true, y_pred, gridsize=60, cmap='plasma', mincnt=1, extent=extent,\n",
    "                   norm=Normalize(vmin=0, vmax=1))\n",
    "    counts = hb.get_array()\n",
    "    norm_counts = counts / counts.max()\n",
    "    hb.set_array(norm_counts)\n",
    "\n",
    "    if label:\n",
    "        ax.set_xlabel(f\"Observed {label}\")\n",
    "        ax.set_ylabel(f\"Predicted {label}\")\n",
    "\n",
    "    if allmetrics:\n",
    "        ax.text(\n",
    "            0.05, 0.95,\n",
    "            f\"R²: {metrics['r2']:.2f}\\nnMAE: {metrics['nmae']:.2f}\\nR: {metrics['pearson_r']:.2f}\",\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white')\n",
    "        )\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "\n",
    "# -------------------- PREPARE DATA --------------------\n",
    "def prepare_data(pred, normval, valmeta, traits):\n",
    "    # normval = pd.read_csv(normval_path)\n",
    "    # pred = pd.read_csv(val_path)\n",
    "    pred = pd.concat([\n",
    "        pred,\n",
    "        normval[[c for c in normval.columns if c.endswith(\"_uncertainty\") and c.split(\"_uncertainty\")[0] in traits]],\n",
    "        valmeta[['Longitude', 'Latitude']]\n",
    "    ], axis=1)\n",
    "    for t in traits:\n",
    "        if t.lower() == \"leafarea\":\n",
    "            pred[f\"{t}_true\"] *= 100\n",
    "            pred[f\"{t}_pred\"] *= 100\n",
    "        #if t.lower() == \"leaf_n then make it leafn\n",
    "        if t.lower() == \"leaf_n\":\n",
    "            pred.rename(columns={f\"{t}_true\": \"LeafN_true\", f\"{t}_pred\": \"LeafN_pred\"}, inplace=True)\n",
    "    return pred\n",
    "\n",
    "\n",
    "# -------------------- SPLOT BENCHMARKING --------------------\n",
    "def splot_benchmarking(pred, cfg, traits, out_dir, n_min=20):\n",
    "    results = []\n",
    "    out_dir = Path(out_dir)\n",
    "\n",
    "    for t in traits:\n",
    "        splot_df = read_trait_map(t, \"splot\", band=1).to_dataframe(name=t).drop(columns=[\"band\", \"spatial_ref\"]).dropna()\n",
    "        \n",
    "        \n",
    "\n",
    "        grid_true = global_grid_df(pred, f\"{t}_true\", lon=\"Longitude\", lat=\"Latitude\", res=1, stats=[\"mean\"], n_min=n_min)\n",
    "        grid_pred = global_grid_df(pred, f\"{t}_pred\", lon=\"Longitude\", lat=\"Latitude\", res=1, stats=[\"mean\"], n_min=n_min)\n",
    "        \n",
    "        print(f\"sPlot df shape for {t}: {splot_df.shape}\")\n",
    "        print(f\"Grid true shape for {t}: {grid_true.shape}\")\n",
    "        print(f\"Grid pred shape for {t}: {grid_pred.shape}\")\n",
    "\n",
    "        # Join on grid index (y, x)\n",
    "        merged = grid_true.join(grid_pred, lsuffix=\"_true\", rsuffix=\"_pred\", how=\"inner\")\n",
    "        merged = merged.join(splot_df, how=\"inner\")\n",
    "        \n",
    "        print(f\"Merged df shape for {t}: {merged.shape}\")\n",
    "        \n",
    "\n",
    "        merged.rename(columns={t: f\"{t}_splot\", \"mean_true\": f\"{t}_true\", \"mean_pred\": f\"{t}_pred\"}, inplace=True)\n",
    "        merged = merged[[f\"{t}_splot\", f\"{t}_true\", f\"{t}_pred\"]].dropna()\n",
    "\n",
    "        if merged.empty:\n",
    "            continue\n",
    "\n",
    "        # Save combined data\n",
    "        combined_path = out_dir / \"combined_data\" / f\"{t}_splot_vs_model_1deg_nmin{n_min}.parquet\"\n",
    "        combined_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        merged.reset_index().to_parquet(combined_path)\n",
    "        print(f\"Saved combined data for {t} at {combined_path}\")\n",
    "        \n",
    "\n",
    "        lat_wts = lat_weights(merged.index.get_level_values(\"y\").unique().values, 1)\n",
    "\n",
    "        # Plotting\n",
    "        with sns.plotting_context(\"notebook\"), sns.axes_style(\"ticks\"):\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "            plothexbin(merged, f\"{t}_splot\", f\"{t}_true\", lat_wts, ax[0],\n",
    "                       label=t, title=\"sPlot vs TRY6 mean\", log_scale=True)\n",
    "\n",
    "            plothexbin(merged, f\"{t}_splot\", f\"{t}_pred\", lat_wts, ax[1],\n",
    "                       label=t, title=\"sPlot vs Model\", log_scale=True)\n",
    "\n",
    "            fig_path = out_dir / \"plots\" / f\"nmin{n_min}\" / f\"{t}_vs_splot_1deg_nmin{n_min}.png\"\n",
    "            fig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            plt.savefig(fig_path, dpi=300)\n",
    "            plt.close(fig)\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = compute_weighted_metrics(merged, lat_wts, f\"{t}_pred\", f\"{t}_splot\", log_scale=True)\n",
    "        metrics.update(trait=t)\n",
    "        results.append(metrics)\n",
    "\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(out_dir / \"splot_metrics.csv\", index=False)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0894a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298501, 8) (298501, 12) (298501, 2)\n",
      "(84710, 8) (84710, 12) (84710, 2)\n",
      "After concatenation:\n",
      "(383211, 8) (383211, 12) (383211, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#running inference.py creates two files, <>.csv which contains trait prediction in original scale and normalized_<>.csv which contains trait predictions with normalized values\n",
    "\n",
    "# === File Paths ===\n",
    "mega_val_path = '<>/results_benchmark_data.csv'\n",
    "meganorm_path = '<>/results_normalized_benchmark_data.csv'\n",
    "megameta_path = '<>/benchmark_data.csv' #meta data provided in the dataset reporsitory\n",
    "\n",
    "\n",
    "val_path = '<>/results_val.csv'\n",
    "normval_path = '<>/results_normalized_val.csv'\n",
    "valmeta_path = '<>/val.csv'\n",
    "\n",
    "# === Load CSVs ===\n",
    "megaval = pd.read_csv(mega_val_path).drop(columns=[\n",
    "    'Height_uncertainty', 'LeafArea_uncertainty', 'SLA_uncertainty', 'Leaf_N_uncertainty'\n",
    "])\n",
    "meganormval = pd.read_csv(meganorm_path)\n",
    "\n",
    "val = pd.read_csv(val_path).drop(columns=[\n",
    "    'Height_uncertainty', 'LeafArea_uncertainty', 'SLA_uncertainty', 'Leaf_N_uncertainty'\n",
    "])\n",
    "normval = pd.read_csv(normval_path)\n",
    "valmeta = pd.read_csv(valmeta_path)[['Longitude', 'Latitude']]\n",
    "\n",
    "#rename latitude and longitude columns to Latitude and Longitude for consistency\n",
    "valmeta = valmeta.rename(columns={'longitude': 'Longitude', 'latitude': 'Latitude'})\n",
    "\n",
    "print(megaval.shape, meganormval.shape, megavalmeta.shape)\n",
    "print(val.shape, normval.shape, valmeta.shape)\n",
    "\n",
    "\n",
    "\n",
    "# === Concatenate Predictions, Normals, and Metadata ===\n",
    "full_pred = pd.concat([megaval, val], axis=0).reset_index(drop=True)\n",
    "full_norm = pd.concat([meganormval, normval], axis=0).reset_index(drop=True)\n",
    "full_meta = pd.concat([megavalmeta, valmeta], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(\"After concatenation:\")\n",
    "\n",
    "print(full_pred.shape, full_norm.shape, full_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b954d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.benchmark_conf import get_benchmark_config\n",
    "cfg = get_benchmark_config()\n",
    "traits=['Height', 'LeafArea', 'SLA', 'Leaf_N']\n",
    "pred = prepare_data(pred=full_pred, normval=ful_norm, valmeta=full_meta, traits=['Height', 'LeafArea', 'SLA', 'LeafN'])\n",
    "            \n",
    "splot_benchmarking(pred, cfg, traits=traits, out_dir='./output', n_min=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
